# -*- coding: utf-8 -*-
"""Model_Eğiyim.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16A58-Ojo6x98lixGweJw9lD8MUwiCWKp
"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_adi = "google/flan-t5-small"

tokenizer = AutoTokenizer.from_pretrained(model_adi)
model = AutoModelForSeq2SeqLM.from_pretrained(model_adi)

# Google Drive'a kaydetmek için:
model.save_pretrained("/content/drive/MyDrive/dj_model_flan_t5")
tokenizer.save_pretrained("/content/drive/MyDrive/dj_model_flan_t5")

# === Kurulumlar ===
!pip install transformers datasets

# === Google Drive bağlantısı ===
from google.colab import drive
drive.mount('/content/drive')

# === Gerekli Kütüphaneler ===
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq
from datasets import Dataset
import pandas as pd

# === Verileri oku ===
train_df = pd.read_csv("/content/600_Sat_rl_k_DJ_Anons_Verisi.csv")

# === Huggingface modelini yükle (flan-t5-small) ===
model_path = "google/flan-t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

# === Dataset nesnelerine dönüştür ===
train_dataset = Dataset.from_pandas(train_df)

# === Tokenleştirme fonksiyonu ===
def tokenize_function(example):
    model_inputs = tokenizer(example["input"], max_length=32, truncation=True, padding="max_length")
    labels = tokenizer(example["target"], max_length=64, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_val = val_dataset.map(tokenize_function, batched=True)

# === Eğitim Ayarları ===
training_args = Seq2SeqTrainingArguments(
    output_dir="/content/drive/MyDrive/dj_model_flan_t5",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    weight_decay=0.01,
    save_total_limit=1,
    num_train_epochs=5,
    predict_with_generate=True,
    logging_dir="/content/drive/MyDrive/dj_model_flan_t5/logs",
    report_to="none"  # wandb kapalı
)

# === Trainer ===
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer, model)
)

# === Eğitimi başlat ===
trainer.train()

# === Eğitilen modeli ve tokenizer'ı kaydet ===
trainer.save_model("/content/drive/MyDrive/dj_model_flan_t5")
tokenizer.save_pretrained("/content/drive/MyDrive/dj_model_flan_t5")